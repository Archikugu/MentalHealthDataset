{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdYOE7ODlSiV"
      },
      "outputs": [],
      "source": [
        "#CSV Dosyasını Okuma:\n",
        "import pandas as pd\n",
        "data = pd.read_csv('mental_health.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmZ72mIGmiQc"
      },
      "outputs": [],
      "source": [
        "#Küçük Harfe Dönüştürme:\n",
        "data['text'] = data['text'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utVJ8w7LmroF"
      },
      "outputs": [],
      "source": [
        "#Noktalama İşaretlerini Kaldırma:\n",
        "import string\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1ahvYi9m_w0"
      },
      "outputs": [],
      "source": [
        "#Sayıları Kaldırma:\n",
        "data['text'] = data['text'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzwha9TlnhiR",
        "outputId": "6c41a11f-e99e-4c57-f5d4-211b0aff3c40"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'him', 'some', 'my', 'not', 'i', 'did', 'should', \"that'll\", 'again', 'an', 'me', 'any', 'we', 'up', \"won't\", 'at', 'such', 'until', 'he', 'both', 'just', 't', 'your', 'o', 'y', 's', 'am', 'off', 'few', \"shan't\", 'where', 'what', 'while', 'through', \"didn't\", 'mustn', 'himself', 'or', 'on', 'won', 'don', 'yourselves', 'was', 'itself', 're', \"aren't\", 'been', 'why', 'themselves', 'are', 'being', \"needn't\", 'about', 'yours', 'hadn', 'will', 'didn', 'after', 'ain', 'shouldn', 'in', 'each', 'it', 'but', \"you'll\", \"weren't\", \"should've\", 'aren', 'during', 'doing', 'which', 'as', 'wouldn', 'mightn', 'is', \"mustn't\", 'wasn', 'her', 'had', 'own', 'having', 'them', 'for', 'between', 'no', 'by', 'then', \"hasn't\", \"hadn't\", \"shouldn't\", 'those', \"you're\", 'does', \"she's\", \"don't\", 'has', 'can', 'you', 'myself', 'shan', 'needn', 'into', 'nor', 'with', 'do', 'to', 'theirs', 'if', 'above', 'll', 'weren', 'against', 'hers', 'the', 'there', \"wasn't\", 'here', \"haven't\", 'than', 'because', \"couldn't\", \"doesn't\", 'were', \"you'd\", 'how', 'other', 'yourself', 'of', 'very', 'd', 'further', 'our', 'ma', 'his', 'isn', 'these', 'more', 'haven', 'once', 'before', 'and', 'doesn', 'their', 'under', 'below', 'same', 'hasn', \"it's\", 'from', 'out', 'a', \"wouldn't\", 'have', 'whom', 'ourselves', 'she', 'its', 'be', \"you've\", 'when', 'all', \"isn't\", 'so', 'too', 've', 'this', 'ours', 'm', 'they', 'couldn', 'most', 'that', 'herself', 'now', 'down', \"mightn't\", 'over', 'only', 'who'}\n"
          ]
        }
      ],
      "source": [
        "#Stop Words'leri Kaldırma:\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "print(stop_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3RtWJ84rKWO"
      },
      "outputs": [],
      "source": [
        "#Özel Karakterleri Kaldırma:\n",
        "data['text'] = data['text'].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VizrVuuirS3y"
      },
      "outputs": [],
      "source": [
        "#Boşlukları Temizleme:\n",
        "data['text'] = data['text'].apply(lambda x: ' '.join(x.split()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shdniyY-Z1UF"
      },
      "outputs": [],
      "source": [
        "#Ngram\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "ngram_range = (2, 2)\n",
        "vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
        "X = vectorizer.fit_transform(data['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdcDVeXmsNm0"
      },
      "outputs": [],
      "source": [
        "#Kök Bulma\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuWpFZ5TrzLf",
        "outputId": "2b434dd4-543f-47ae-a6df-1efa4e61acd2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Lemmatizasyon\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sVfNC96mfqh",
        "outputId": "d73b55b8-3b91-4163-ba6d-cffcf2d5f116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                    text  label\n",
            "0      dear american teen question dutch person heard...      0\n",
            "1      noth look forward lifei dont mani reason keep ...      1\n",
            "2      music recommend im look expand playlist usual ...      0\n",
            "3      im done tri feel betterth reason im still aliv...      1\n",
            "4      worri year old girl subject domest physicalmen...      1\n",
            "...                                                  ...    ...\n",
            "27972  post everyday peopl stop care religion matter ...      0\n",
            "27973  okay definetli need hear guy opinion ive prett...      0\n",
            "27974  cant get dog think ill kill myselfth last thin...      1\n",
            "27975  what point princess bridei realli think like w...      1\n",
            "27976  got nude person might might know snapchat ok c...      0\n",
            "\n",
            "[27977 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "print(data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-a8J262yNWpv"
      },
      "source": [
        "**Vektörizasyon İşlemleri**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AQtV5SNFFWKQ"
      },
      "source": [
        "**Bag of Words kullanarak vektörizasyon**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tALR9a7o-Lns"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Metinleri alıyoruz\n",
        "texts = data['text']\n",
        "\n",
        "# Etiketleri alıyoruz\n",
        "y = data['label']\n",
        "\n",
        "# Bag-of-Words vektörleştirme yaptık\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B2pghKJJFRwA"
      },
      "source": [
        "**TF-IDF kullanarak vektörizasyon**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKW_Z2R3mh78"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# metin ve etiketlerini ayırıyoruz\n",
        "yorumlar = data['text'].tolist()\n",
        "y = data['label'].tolist()\n",
        "\n",
        "# TF-IDF algoritmasını tanımlıyoruz\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Metin verilerini vektörleştirip X değişkenine atıyoruz\n",
        "X = vectorizer.fit_transform(yorumlar)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-GKJ34tyNOfR"
      },
      "source": [
        "**Model Eğitimi**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OFoZDWrsW3R"
      },
      "outputs": [],
      "source": [
        "#Model eğitimi için kütüphaneleri ekliyoruz\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras import backend \n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import BernoulliNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naiXZXZXvg_N"
      },
      "outputs": [],
      "source": [
        "# X ve y değerlerini test ve train olarak ayırıyoruz\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_T3CkOg9EKfi"
      },
      "source": [
        "**Naive Bayes ile model eğitimi**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XKoRN6xEDPi"
      },
      "outputs": [],
      "source": [
        "# Naive Bayes modelini oluşturuyoruz\n",
        "#model = MultinomialNB()\n",
        "model = BernoulliNB()\n",
        "\n",
        "# Modeli eğitim verileriyle eğitiyoruz\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5IUOuIyTM1nv"
      },
      "source": [
        "**SVM ile Model Eğitimi**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4QUy3FPwcI0"
      },
      "outputs": [],
      "source": [
        "# SVM modelini tanımlayıp eğitiyoruz.\n",
        "svm =SVC(kernel='sigmoid')\n",
        "svm.fit(X_train, y_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3BWhNqfUUiN_"
      },
      "source": [
        "**CNN ile Model Eğitimi**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQndx99tUi2p"
      },
      "outputs": [],
      "source": [
        "# Etiketleri kategorik hale getiriyoruz\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "y_train_categorical = np_utils.to_categorical(y_train_encoded)\n",
        "y_test_categorical = np_utils.to_categorical(y_test_encoded)\n",
        "# Verileri sıfır dolgu (zero-padding) ile aynı uzunluğa getiriyoruz\n",
        "max_sequence_length = X.shape[1]  # TF-IDF matrisinin sütun sayısı\n",
        "X_train_padded = pad_sequences(X_train.toarray(), maxlen=max_sequence_length)\n",
        "X_test_padded = pad_sequences(X_test.toarray(), maxlen=max_sequence_length)\n",
        "# CNN modelini oluşturuyoruz\n",
        "model = Sequential()\n",
        "model.add(Embedding(X.shape[1], 64, input_length=max_sequence_length))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "# Modeli derliyoruz\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Modeli eğitiyoruz\n",
        "model.fit(X_train_padded, y_train_categorical, epochs=10, batch_size=32)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BJmbBj-eM7qr"
      },
      "source": [
        "**Model Başarımları**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCv_KMxvDjie",
        "outputId": "d93d3c18-9543-4127-dc8b-aaefadf28351"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Doğruluk (Accuracy): 0.9245889921372409\n",
            "F1 skoru: 0.9240460763138949\n"
          ]
        }
      ],
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Doğruluk (Accuracy):\", accuracy)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(\"F1 skoru:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9k9Qeshweis",
        "outputId": "f31b3d99-607e-41d4-bb1c-764fac481b5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Doğruluk (Accuracy): 0.9253037884203003\n",
            "F1 skoru: 0.9247389268995317\n"
          ]
        }
      ],
      "source": [
        "y_test_pred = svm.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(\"Doğruluk (Accuracy):\", test_accuracy)\n",
        "y_test_pred = svm.predict(X_test)\n",
        "f1 = f1_score(y_test, y_test_pred)\n",
        "print(\"F1 skoru:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jcJqkDi4XHM",
        "outputId": "ca671250-d997-4212-f4a7-a231f92b5727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[2610  192]\n",
            " [ 226 2568]]\n"
          ]
        }
      ],
      "source": [
        "# Karışıklık matrisi\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred = svm.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "# TP-FN/FP-TN"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
